{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc99efb6-407b-47c2-bb94-71cb5d2a9c84",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Recurrent Neural Networks are designed to learn sequence data with temporal dependencies such as speech and other time series. Recurrent Neural Networks take the time dimension into account by introducing a recursive connection with a time delay of -1.\n",
    "\n",
    "\n",
    "The idea behind RNNs is to use sequential information. In a feedforward neural network we assume that all inputs (and outputs) are independent of each other, but especially in finance that is a bad assumption. If you want to predict future returns it is probably better to know previous information about past returns of the same security. RNNs are called recurrent because they perform the same task for every element of a sequence, with the output being depended on the previous computations and you already know that they have a *memory* which captures information about what has been calculated so far. A RNN module is presented in the figure below\n",
    "\n",
    "<img src=\"images/unrolledRNN.png\" width=\"500\">\n",
    "\n",
    "One can implement this module either with `tensorflow` or `pytorch`. The unit can be considered as a replacement for a single neuron that have a feedback loop in addition. *This way the model is able to consider the time*.\n",
    "\n",
    "The core reason that recurrent nets are more exciting is that they allow us to operate over sequences of vectors: Sequences in the input, the output, or in the most general case both. There are different examples of RNNs:\n",
    "\n",
    "<img src=\"images/rnntypes.png\" width=\"500\">\n",
    "\n",
    "\n",
    "However, there are different recurrent units that are known to perform better in practice. Why is that? We can find a detailed answer about drawbacks of RNN module and its extensions [here](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "Any references for Recurrent Neural Networks can be found in Goodfellow's book, which has a dedicate chapter to this neural network family.\n",
    "\n",
    "Let's now dive into the code, to see how we need to prepare data to be ingested by a RNN and what are the pros and cons of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9b7fcf-9ffd-4ac9-9436-bb3b9f5fe27c",
   "metadata": {},
   "source": [
    "# Time series forecasting using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb9b4a-fca8-4e5d-88b4-c6ac6b62f183",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T01:50:57.970592Z",
     "start_time": "2020-01-29T01:50:56.700367Z"
    }
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random,os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, Subset\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f700336e-311f-472d-a2df-9f46997dba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_data(series, n_lags=1, n_leads=1):\n",
    "    '''\n",
    "    Function for transforming time series into input acceptable by a multilayer perceptron.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    series : np.array\n",
    "        The time series to be transformed\n",
    "    n_lags : int\n",
    "        The number of lagged observations to consider as features\n",
    "    n_leads : int\n",
    "        The number of future periods we want to forecast for\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X : np.array\n",
    "        Array of features\n",
    "    y : np.array\n",
    "        Array of target\n",
    "    '''\n",
    "    X = []\n",
    "    y = []\n",
    "    for step in range(len(series) - n_lags - n_leads + 1):\n",
    "        end_step = step + n_lags\n",
    "        forward_end = end_step + n_leads\n",
    "        X.append(series[step:end_step])\n",
    "        y.append(series[end_step:forward_end])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# custom function for reproducibility\n",
    "\n",
    "\n",
    "def custom_set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18be850b-1d94-4ded-ace7-27a3b6466f64",
   "metadata": {},
   "source": [
    "Define the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2708a0-ffd8-457e-a6aa-450823583477",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T01:50:57.994763Z",
     "start_time": "2020-01-29T01:50:57.991765Z"
    }
   },
   "outputs": [],
   "source": [
    "# data\n",
    "TICKER = 'AAPL'\n",
    "START_DATE = '2010-01-02'\n",
    "END_DATE = '2019-12-31'\n",
    "VALID_START = '2019-07-01'\n",
    "N_LAGS = 12\n",
    "\n",
    "# neural network \n",
    "BATCH_SIZE = 16\n",
    "N_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee0d27f-bfc7-46d9-8b9b-c671f87a00c4",
   "metadata": {},
   "source": [
    "Download and prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a81b000-1abe-4a4c-9f7d-a579b00f943a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T01:50:58.539057Z",
     "start_time": "2020-01-29T01:50:58.332943Z"
    }
   },
   "outputs": [],
   "source": [
    "df = yf.download(TICKER, \n",
    "                 start=START_DATE, \n",
    "                 end=END_DATE,\n",
    "                 progress=False)\n",
    "\n",
    "df = df.resample('W-MON').last() # weekly frequency from Monday\n",
    "valid_size = df.loc[VALID_START:END_DATE].shape[0]\n",
    "prices = df['Adj Close'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9913cdd-a74a-4363-8176-d33e17aeac2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T01:51:02.306178Z",
     "start_time": "2020-01-29T01:50:58.625943Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(df.index, prices)\n",
    "ax.set(title=f\"{TICKER}'s Stock price\", \n",
    "       xlabel='Time', \n",
    "       ylabel='Price ($)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adf3fff-42f8-4a51-a51d-5c6be2f27851",
   "metadata": {},
   "source": [
    "Scale the time series of prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1902d3-88b1-4650-a2a9-3e4fafb75864",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T01:51:02.355950Z",
     "start_time": "2020-01-29T01:51:02.351905Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_ind = len(prices) - valid_size\n",
    "\n",
    "minmax = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "prices_train = prices[:valid_ind]\n",
    "prices_valid = prices[valid_ind:]\n",
    "\n",
    "minmax.fit(prices_train)\n",
    "\n",
    "prices_train = minmax.transform(prices_train)\n",
    "prices_valid = minmax.transform(prices_valid)\n",
    "\n",
    "prices_scaled = np.concatenate((prices_train, \n",
    "                                prices_valid)).flatten()\n",
    "#plt.plot(prices_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3189a8cb-a33b-42b1-82b5-a09094252dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203caff8-8dda-4f5a-9c34-bbcbfd5e4a41",
   "metadata": {},
   "source": [
    "Transform the time series into input for the RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb52926e-fef5-4a08-ba9f-f0b75f01f56d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T17:13:42.077002Z",
     "start_time": "2020-01-17T17:13:42.070559Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = create_input_data(prices_scaled, N_LAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059322a3-4a18-43e5-a7ca-65ac52a16dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd87da9-af17-494c-a654-e7e9ce3f1a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15cb151-7615-4aa3-816d-1a8108ffb7fe",
   "metadata": {},
   "source": [
    "Obtain the na√Øve forecast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e28291a-8fac-47a4-baf5-e81a5c74ffa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T17:13:42.172555Z",
     "start_time": "2020-01-17T17:13:42.167345Z"
    }
   },
   "outputs": [],
   "source": [
    "naive_pred = prices[len(prices)-valid_size-1:-1]\n",
    "y_valid = prices[len(prices)-valid_size:]\n",
    "\n",
    "naive_mse = mean_squared_error(y_valid, naive_pred)\n",
    "naive_rmse = np.sqrt(naive_mse)\n",
    "print(f\"Naive forecast - MSE: {naive_mse:.4f}, RMSE: {naive_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab591b9d-c4ce-44dc-befa-20ae5ee10bf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T07:22:50.333717Z",
     "start_time": "2020-01-13T07:22:50.330143Z"
    }
   },
   "source": [
    "Prepare the `DataLoader` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff41de41-4fd4-4a54-b6df-351a0799cfdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T17:13:46.110100Z",
     "start_time": "2020-01-17T17:13:46.095568Z"
    }
   },
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "custom_set_seed(42)\n",
    "\n",
    "valid_ind = len(X) - valid_size\n",
    "\n",
    "X_tensor = torch.from_numpy(X).float().reshape(X.shape[0], \n",
    "                                               X.shape[1], \n",
    "                                               1)\n",
    "y_tensor = torch.from_numpy(y).float().reshape(X.shape[0], 1)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "train_dataset = Subset(dataset, list(range(valid_ind)))\n",
    "valid_dataset = Subset(dataset, list(range(valid_ind, len(X))))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, \n",
    "                          batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bfabbd-d92e-4f28-ac73-c52d2c535f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dafb89-cbb1-4b9b-a6bb-f9b85c23323e",
   "metadata": {},
   "source": [
    "Check the size of the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5253d88-5db5-4f4d-bc38-0d6f24adbb20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T17:13:47.158485Z",
     "start_time": "2020-01-17T17:13:47.155571Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Size of datasets - training: {len(train_loader.dataset)} | validation: {len(valid_loader.dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497bd04b-eeda-4943-be4c-6c25fc831af1",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf1a3d-eacc-4502-9cfc-14db72505249",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T17:13:51.793811Z",
     "start_time": "2020-01-17T17:13:51.788377Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, \n",
    "                          n_layers, batch_first=True,\n",
    "                          nonlinearity='relu')\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        output = self.fc(output[:,-1,:]) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78dec4d-d45b-4dec-8a6d-d440e437db05",
   "metadata": {},
   "source": [
    "Instantiate the model, the loss function and the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd90b7cd-186a-446b-b187-0e2e551fef8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T17:13:54.680163Z",
     "start_time": "2020-01-17T17:13:54.675700Z"
    }
   },
   "outputs": [],
   "source": [
    "model = RNN(input_size=1, hidden_size=6, \n",
    "            n_layers=1, output_size=1).to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f69cb1-bfa9-4580-84a7-1a6284333757",
   "metadata": {},
   "source": [
    "Train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66cebfc-0b72-4f4b-b184-1e16f488c0fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T17:14:03.033487Z",
     "start_time": "2020-01-17T17:13:55.851477Z"
    }
   },
   "outputs": [],
   "source": [
    "PRINT_EVERY = 10\n",
    "train_losses, valid_losses = [], []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    running_loss_train = 0\n",
    "    running_loss_valid = 0\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for x_batch, y_batch in train_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        y_hat = model(x_batch)\n",
    "        loss = torch.sqrt(loss_fn(y_batch, y_hat))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss_train += loss.item() * x_batch.size(0)\n",
    "        \n",
    "    epoch_loss_train = running_loss_train / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_loss_train)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for x_val, y_val in valid_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            y_hat = model(x_val)\n",
    "            loss = torch.sqrt(loss_fn(y_val, y_hat))\n",
    "            running_loss_valid += loss.item() * x_val.size(0)\n",
    "            \n",
    "        epoch_loss_valid = running_loss_valid / len(valid_loader.dataset)\n",
    "            \n",
    "        if epoch > 0 and epoch_loss_valid < min(valid_losses):\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), './outputs/rnn_checkpoint.pth')\n",
    "            \n",
    "        valid_losses.append(epoch_loss_valid)\n",
    "\n",
    "    if epoch % PRINT_EVERY == 0:\n",
    "        print(f\"<{epoch}> - Train. loss: {epoch_loss_train:.4f} \\t Valid. loss: {epoch_loss_valid:.4f}\")\n",
    "        \n",
    "print(f'Lowest loss recorded in epoch: {best_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f08ba5-f9bc-4e29-94fb-8f7f7b23fc98",
   "metadata": {},
   "source": [
    "Plot the losses over epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f2a43-c96b-4505-b683-d667a7240924",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T17:15:44.015092Z",
     "start_time": "2020-01-17T17:15:38.838476Z"
    }
   },
   "outputs": [],
   "source": [
    "train_losses = np.array(train_losses)\n",
    "valid_losses = np.array(valid_losses)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(train_losses, color='blue', label='Training loss')\n",
    "ax.plot(valid_losses, color='red', label='Validation loss')\n",
    "\n",
    "ax.set(title=\"Loss over epochs\", \n",
    "       xlabel='Epoch', \n",
    "       ylabel='Loss')\n",
    "ax.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('images/ch10_im14.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c98a4a2-46ef-4775-8bb5-2f1a2ca9540b",
   "metadata": {},
   "source": [
    "Load the best model (with the lowest validation loss):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2314bc2a-5db4-4b4d-b3cb-5e10190f2838",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T17:15:44.169860Z",
     "start_time": "2020-01-17T17:15:44.165410Z"
    }
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load('outputs/rnn_checkpoint.pth')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124509f3-e971-4c11-8c22-ebcba72531ba",
   "metadata": {},
   "source": [
    "Obtain the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989bca7c-d8dd-4da9-90ef-8040d30ac936",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T17:15:45.357654Z",
     "start_time": "2020-01-17T17:15:45.350744Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for x_val, y_val in valid_loader:\n",
    "        x_val = x_val.to(device)\n",
    "        y_hat = model(x_val)\n",
    "        y_pred.append(y_hat)\n",
    "        \n",
    "y_pred = torch.cat(y_pred).numpy()\n",
    "y_pred = minmax.inverse_transform(y_pred).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3333f4-ee78-4fab-85b8-23b406a1ba93",
   "metadata": {},
   "source": [
    "Evaluate the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9e3af6-40dd-4c48-9d21-e1caf01a8fc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T17:16:10.556651Z",
     "start_time": "2020-01-17T17:16:04.261880Z"
    }
   },
   "outputs": [],
   "source": [
    "rnn_mse = mean_squared_error(y_valid, y_pred)\n",
    "rnn_rmse = np.sqrt(rnn_mse)\n",
    "print(f\"RNN's forecast - MSE: {rnn_mse:.4f}, RMSE: {rnn_rmse:.4f}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "ax.plot(y_valid, color='blue', label='Actual')\n",
    "ax.plot(y_pred, color='red', label='RNN')\n",
    "ax.plot(naive_pred, color='green', label='Na√Øve')\n",
    "\n",
    "ax.set(title=\"RNN's Forecasts\", \n",
    "       xlabel='Time', \n",
    "       ylabel='Price ($)')\n",
    "ax.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b08c23-a2cb-4644-9ee2-4b03fb14ad25",
   "metadata": {},
   "source": [
    "# Time series forecasting using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf9786-a2cf-4412-b76e-72aeb3faf01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import sys, os\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6ce949-e113-4c4a-894b-a445b3a88a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(data,\n",
    "              date_features=None,\n",
    "              add_cyclic_date=False,\n",
    "              lookback=30,\n",
    "              transformer_x=None,\n",
    "              use_transformer=False,\n",
    "              # return_time_idx=False,\n",
    "              rolling_split=False,\n",
    "              verbose=False):\n",
    "    \"\"\"\n",
    "    Transform inputs to 3-D tensors\n",
    "    and y as the one time step ahead.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        * data: data to create time series targets and features for\n",
    "        LSTM.\n",
    "        * lookback:\n",
    "\n",
    "    ---\n",
    "    Shape of data:\n",
    "        features: (total trading days, history for regression, no of features)\n",
    "        labels: (total trading days, no of features)\n",
    "\n",
    "    Return:\n",
    "    \"\"\"\n",
    "\n",
    "    if add_cyclic_date:\n",
    "        x = np.concatenate((data, date_features), axis=1)\n",
    "    # just do a copy of same data\n",
    "    else:\n",
    "        x = data\n",
    "    y = data\n",
    "\n",
    "    # repeat this for train-val-test\n",
    "    xtrain, xval, ytrain, yval = train_test_split(\n",
    "        x, y, shuffle=False, random_state=42, test_size=0.25)\n",
    "\n",
    "    xval, xtest, yval, ytest = train_test_split(\n",
    "        xval, yval, shuffle=False, random_state=42, test_size=0.5)\n",
    "\n",
    "    imputer_x = SimpleImputer(strategy='median')\n",
    "    xtrain = imputer_x.fit_transform(xtrain)\n",
    "    xval = imputer_x.transform(xval)\n",
    "    xtest = imputer_x.transform(xtest)\n",
    "\n",
    "    imputer_y = SimpleImputer(strategy='median')\n",
    "    ytrain = imputer_y.fit_transform(ytrain)\n",
    "    yval = imputer_y.transform(yval)\n",
    "    ytest = imputer_y.transform(ytest)\n",
    "\n",
    "    if use_transformer:\n",
    "        if transformer_x is None:\n",
    "            transformer_x = MinMaxScaler()\n",
    "            xtrain = transformer_x.fit_transform(xtrain)\n",
    "            xval = transformer_x.transform(xval)\n",
    "            xtest = transformer_x.transform(xtest)\n",
    "\n",
    "    xtrain, ytrain = helper_train_test(xtrain, ytrain, lookback)\n",
    "    xval, yval = helper_train_test(xval, yval, lookback)\n",
    "    xtest, ytest = helper_train_test(xtest, ytest, lookback)\n",
    "\n",
    "    return xtrain, xval, xtest, ytrain, yval, ytest\n",
    "\n",
    "\n",
    "def data_pipe(df,\n",
    "              transformer_x=None,\n",
    "              use_transformer=False,\n",
    "              # return_time_idx=True,\n",
    "              use_tf_data=False,\n",
    "              add_cyclic_date=False):\n",
    "    \"\"\"Data pipe splits data in train-val-test, then\n",
    "    it does preprocessing on it. This logic might be implemented\n",
    "    on the layes itself.\n",
    "\n",
    "    Args:\n",
    "        * df: dataframe with data to train.\n",
    "        * transformer: use a data transformer for\n",
    "        preprocessing.\n",
    "        * use_tf_data: if True use the tf-data-set class.\n",
    "    ---\n",
    "\n",
    "\n",
    "    Return\n",
    "    ---\n",
    "    A dictionary with each of train, val and test sets.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \"\"\" split should be done in the to_tensor function \"\"\"\n",
    "    # df_train, df_val, df_test = time_series_split(df)\n",
    "    # TODO: get time indexes here\n",
    "    # concat those inside the to_tensor function\n",
    "    # transform to sine\n",
    "    if add_cyclic_date:\n",
    "        data_datefeatures = add_cyclic_datepart(add_datefeatures(df))\n",
    "\n",
    "        xtrain, xval, xtest, ytrain, yval, ytest = to_tensor(\n",
    "            df, data_datefeatures, use_transformer=use_transformer)\n",
    "    else:\n",
    "        xtrain, xval, xtest, ytrain, yval, ytest = to_tensor(\n",
    "            df, use_transformer=use_transformer)\n",
    "\n",
    "    if use_tf_data:\n",
    "        data_train, data_val, data_test = train_val_tf(\n",
    "            xtrain, ytrain, xval, yval, xtest, ytest)\n",
    "\n",
    "        return dict(data_train=data_train,\n",
    "                    data_val=data_val,\n",
    "                    data_test=data_test)\n",
    "\n",
    "    return dict(xtrain=xtrain, ytrain=ytrain,\n",
    "                xval=xval, yval=yval,\n",
    "                xtest=xtest, ytest=ytest)\n",
    "\n",
    "def helper_train_test(data_x, data_y, lookback):\n",
    "    \"\"\"Helper function for the creation of time series data\"\"\"\n",
    "    x, y = [], []\n",
    "\n",
    "    time_length = len(data_x)\n",
    "\n",
    "    for i in range(time_length - lookback):\n",
    "        x.append(data_x[i: i + lookback])\n",
    "        y.append(data_y[i + lookback])\n",
    "\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aaf15d-319b-48eb-903a-08603ba0eb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/closing_prices.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "\n",
    "# df.index.rename('Date', inplace=True)\n",
    "# df.rename(columns={'Unnamed: 0', 'Date'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20aeee7-a4a4-4b6d-8965-91c9c11d9a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, :10].plot(figsize=(15, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ea070a-b512-499a-9225-015c266b72de",
   "metadata": {},
   "source": [
    "Train model on 10 stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e69cfc-0489-4fe4-845a-ae1dfadaf445",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dd3db1-5740-43dd-8088-952196386765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layered_rnn(\n",
    "    units=20, \n",
    "    input_shape=1, \n",
    "    output_shape=1, \n",
    "    learning_rate=0.01\n",
    "):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.SimpleRNN(\n",
    "        units,\n",
    "        return_sequences=True,\n",
    "        input_shape=[None, input_shape])\n",
    "             )\n",
    "    \n",
    "    model.add(tf.keras.layers.SimpleRNN(units))\n",
    "    model.add(tf.keras.layers.Dense(output_shape))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.RMSprop(learning_rate),\n",
    "        loss='mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8833a52-4eeb-494c-9d68-f25de2f682fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model):\n",
    "    metrics_df = pd.DataFrame()\n",
    "\n",
    "    optim_param_dict = {}\n",
    "\n",
    "    for c in df.columns:\n",
    "        optim_param = pd.DataFrame()\n",
    "        if df.loc[:, c].isnull().sum()/len(df) < 0.5:\n",
    "            df.loc[:, c].plot(title=f'{c}');\n",
    "            plt.show();\n",
    "            print(c)\n",
    "            print(df[c].shape)\n",
    "            \n",
    "            first_valid = df.loc[:, c].first_valid_index()\n",
    "            \n",
    "            data_dict = data_pipe(\n",
    "                df.loc[first_valid:, c].values.reshape(-1, 1), \n",
    "                use_tf_data=False,\n",
    "                use_transformer=True\n",
    "            )\n",
    "\n",
    "            xtrain, ytrain, xval, yval, xtest, ytest = (\n",
    "                data_dict['xtrain'], data_dict['ytrain'], \n",
    "                data_dict['xval'], data_dict['yval'],  \n",
    "                data_dict['xtest'], data_dict['ytest']\n",
    "            )\n",
    "\n",
    "            num_outputs = ytrain.shape[-1]\n",
    "            hyper_lstm = None\n",
    "            model = None\n",
    "            model = two_layered_rnn()\n",
    "\n",
    "            history = model.fit(xtrain,\n",
    "                            ytrain,\n",
    "                            batch_size=128,\n",
    "                            epochs=20,\n",
    "                            validation_data=(xval, yval),\n",
    "                            verbose=1)\n",
    "            \n",
    "            pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "            # plt.gca().set_ylim(0, 500)\n",
    "            plt.show();\n",
    "\n",
    "            print('#' * 50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772461a7-0f55-42b3-94df-42de12dae5a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = two_layered_rnn()\n",
    "training_loop(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59792b38-4c4b-42e2-9e64-deff8782b6a4",
   "metadata": {},
   "source": [
    "## Alternative to standard RNNs \n",
    "\n",
    "RNNs suffer from exploding or vanishing gradients. RNNs can have a hard time to learn long term dependencies.\n",
    "\n",
    "**Solutions**:\n",
    "* Exploding gradients can be addressed by gradient clipping\n",
    "* Vanishing gradients can be addressed by gater recurrent units\n",
    "\n",
    "\n",
    "\n",
    "**Examples of gated recurrent units**:\n",
    "* Long Short Term Memory Networks (LSTM)\n",
    "\n",
    "<img src=\"images/lstm.jpg\" width=\"600\">\n",
    "\n",
    "* Gated Recurrent Unit (GRU)\n",
    "\n",
    "<img src=\"images/gru.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df39c299-6e21-4234-beda-d12d4305e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layered_lstm(units=20, input_shape=1, output_shape=1, learning_rate=0.01):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.LSTM(\n",
    "        units,\n",
    "        return_sequences=True,\n",
    "        input_shape=[None, input_shape])\n",
    "             )\n",
    "    \n",
    "    model.add(tf.keras.layers.LSTM(units))\n",
    "    model.add(tf.keras.layers.Dense(output_shape))\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.RMSprop(learning_rate),\n",
    "        loss=tf.keras.losses.Huber(),\n",
    "        metrics=['mae', 'mse'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a9ab20-e91d-4aac-a739-18911ab177a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = two_layered_lstm()\n",
    "training_loop(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32729f5a-0bda-49c3-a9d2-faded0629ab6",
   "metadata": {},
   "source": [
    "# Optional: Working on another dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16beb9bc-4509-4ea6-9f2c-9cca2f375568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import keras\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras import regularizers\n",
    "from scipy.ndimage.interpolation import shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428e0da0-ccad-4b34-add9-238b8b86e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = pd.read_csv('data/closing_prices_tiingo.csv')\n",
    "cls.set_index('date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb9117d-6c46-4106-b93e-b1a40d268195",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cls.head())\n",
    "print(cls.iloc[0:1, 1].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34395de-0210-43a9-a0e7-9e8a62f9ad11",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Functions to preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c880bbce-193d-4355-bbb6-0a2cefc2e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(x_train, x_dev, x_test, normalize=False):\n",
    "    \"\"\" Do imputing and scaling in two steps. If done in\n",
    "    pipeline then it is not possible to inverse-transform.\n",
    "    No need to inverse transform imputing\"\"\"\n",
    "\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    x_train = imputer.fit_transform(x_train)\n",
    "    x_dev = imputer.transform(x_dev)\n",
    "    x_test = imputer.transform(x_test)\n",
    "\n",
    "    if normalize:\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        train_x = scaler.fit_transform(x_train)\n",
    "        dev_x = scaler.transform(x_dev)\n",
    "        test_x = scaler.transform(x_test)\n",
    "    else:\n",
    "        train_x = x_train\n",
    "        dev_x = x_dev\n",
    "        test_x = x_test\n",
    "        scaler = None\n",
    "\n",
    "    return train_x, dev_x, test_x, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc62723-b2d6-4ce3-8fb8-db3ff90868db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_dev_test(seq, dev=0.85,\n",
    "                         timesteps=30,\n",
    "                         normalize=False,\n",
    "                         to_ret=False,\n",
    "                         differenced=False):\n",
    "\n",
    "    x_train, x_dev, x_test, names = get_train_dev_test(seq, dev=dev)\n",
    "    print('shape of train, dev and test sets:',\n",
    "          x_train.shape, x_dev.shape, x_test.shape)\n",
    "\n",
    "    # code added 2018-03-23\n",
    "    if differenced:\n",
    "        x_train = x_train.diff()\n",
    "        x_dev = x_dev.diff()\n",
    "        x_test = x_test.diff()\n",
    "\n",
    "    if to_ret:\n",
    "        x_train = x_train.apply(to_return)\n",
    "        x_dev = x_dev.apply(to_return)\n",
    "        x_test = x_test.apply(to_return)\n",
    "\n",
    "    train_x, dev_x, test_x, scaler = transform_data(x_train,\n",
    "                                                    x_dev, x_test,\n",
    "                                                    normalize=normalize)\n",
    "\n",
    "    train_x = to_tensor(train_x, timesteps=timesteps)\n",
    "    x_train = train_x[:, :-1, :]\n",
    "    y_train = train_x[:, -1]\n",
    "\n",
    "    dev_x = to_tensor(dev_x, timesteps=timesteps)\n",
    "    x_dev = dev_x[:, :-1, :]\n",
    "    y_dev = dev_x[:, -1]\n",
    "\n",
    "    test_x = to_tensor(test_x, timesteps=timesteps)\n",
    "    x_test = test_x[:, :-1, :]\n",
    "    y_test = test_x[:, -1]\n",
    "    print('printing from the split-train-dev-test function:')\n",
    "    print('y_dev raw data:')\n",
    "#    print(scaler.inverse_transform(y_dev))\n",
    "\n",
    "    return x_train, y_train, x_dev, y_dev, x_test, y_test, scaler, names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5ce9ec-6ed4-489f-8929-f7872858c972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_return(x, period=1):\n",
    "    \"\"\" This function supposes that the input is a\n",
    "    dataframe\"\"\"\n",
    "    x_shifted = x.shift(periods=period, axis='index')\n",
    "    return (x - x_shifted)/x_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b696e28c-8321-4819-9ed7-fa70dbb06021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(data, timesteps=30):\n",
    "    x = np.array([data[i:i + timesteps]\n",
    "                  for i in range(len(data) - timesteps)], dtype=float)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc26c29-ebc8-48cd-a246-4c74405fceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dev_test(data_x, dev=0.85, drop_col=0.05):\n",
    "    \"\"\" Split data in\n",
    "    - training,\n",
    "    - development\n",
    "    - test data (also called live data)\n",
    "    \"\"\"\n",
    "    \"\"\" We choose to keep the last 10 % of the data\n",
    "    as test data (live trading, live data). The tests\n",
    "    are performed when the model is finnished training\"\"\"\n",
    "\n",
    "    # Check if 10% or more are NAs if so drop those stocks\n",
    "    # inform how many are droped and how many are left.\n",
    "    dropped_stocks = []\n",
    "    for col in data_x.columns:\n",
    "        if data_x[col].isnull().sum()/len(data_x) > drop_col:\n",
    "            dropped_stocks.append(col)\n",
    "            data_x = data_x.drop(col, axis=1)\n",
    "            print('Stock {} has been dropped as it had more than {} % Nas'.\\\n",
    "                  format(col, drop_col * 100))\n",
    "\n",
    "    print('Number of stocks dropped:', len(dropped_stocks))\n",
    "    print('Number of stocks that are kept: ', len(data_x.columns))\n",
    "\n",
    "    test_idx = int(0.9 * len(data_x))\n",
    "    x_test = data_x.iloc[test_idx:, :]\n",
    "\n",
    "    \"\"\"Get the first 90% of the data for train-dev\"\"\"\n",
    "    train_dev_x = data_x.iloc[:test_idx]\n",
    "\n",
    "    dev_idx = int(dev * len(train_dev_x))\n",
    "    x_train = train_dev_x.iloc[:dev_idx, :]\n",
    "    x_dev = train_dev_x.iloc[dev_idx:, :]\n",
    "\n",
    "    return x_train, x_dev, x_test, data_x.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259ecd75-7aa0-4afe-b5c3-81e9317be246",
   "metadata": {},
   "source": [
    "## Functions for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c211f2-18c5-4120-af20-61b22b750f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compare(y_dev, dev_predict):\n",
    "    k = y_dev.shape[1]\n",
    "    start = 0\n",
    "    step = 5\n",
    "    end = step\n",
    "    for i in range(0, k, step):\n",
    "        ax = dev_predict.iloc[:, start:end].plot(\n",
    "                subplots=True,\n",
    "                figsize=(15, 20),\n",
    "                label='Predictions on dev-set', color='DarkBlue')\n",
    "\n",
    "        y_dev.iloc[:, start:end].plot(ax=ax, subplots=True, figsize=(15, 20),\n",
    "                  title='Real data together with predictions', label='real data',\n",
    "                  color='DarkGreen')\n",
    "        end += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7201c363-3dbb-4e22-b322-3c8dcc1af3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_curves(history):\n",
    "    f1, axarr1 = plt.subplots(2, 1, sharex=True, figsize=(8, 10))\n",
    "    axarr1[0].plot(history.history['loss'])\n",
    "    axarr1[0].set_title('Training Loss')\n",
    "    axarr1[1].plot(history.history['val_loss'])\n",
    "    axarr1[1].set_title('Dev Loss')\n",
    "    axarr1[1].set_xlabel('Epochs')\n",
    "#    f1.suptitle('MSE for stock: {}'.format(st_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a9d2e-ddd7-4861-bb8d-a0accabbb73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, BatchNormalization\n",
    "from sklearn.svm import SVR\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf22af5-9a6f-498a-802d-a700945516ae",
   "metadata": {},
   "source": [
    "## Build the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cbb2f8-9fa2-4909-9526-dad90062b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(drop_rate, lr, units, decay,\n",
    "                look_back,\n",
    "                no_features=1,\n",
    "                no_outputs=1):\n",
    "    \"\"\" Arguments\n",
    "    drop_rate -- drop_rate in dropout\n",
    "    learning_rate -- learning rate\n",
    "    time_steps -- time steps for the sequence\n",
    "    units -- number of hidden units or neurons\n",
    "    decay -- proportion of decay for learning rate\n",
    "    no_features -- number of features, default 1 for a 1-dim time series\n",
    "    no_outputs -- number of targets or outputs from the model\n",
    "    \"\"\"\n",
    "    optim = tf.keras.optimizers.Adam(lr=lr,\n",
    "                            beta_1=0.9,\n",
    "                            beta_2=0.999,\n",
    "                            decay=1e-6, \n",
    "                            clipnorm=1.0)\n",
    "\n",
    "    # TODO: how to implement many layers in keras?\n",
    "    ret_seq = False\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_shape=(look_back, no_features)))\n",
    "    model.add(LSTM(units,input_shape=(look_back, no_features),\n",
    "                   return_sequences=ret_seq,\n",
    "                   kernel_regularizer=regularizers.l2(0.01),\n",
    "                   recurrent_regularizer=regularizers.l2(0.01),\n",
    "                   activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(drop_rate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(no_outputs,\n",
    "                    kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=optim,\n",
    "                  metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad06a84-aa0a-46d2-9f02-651e733f06c4",
   "metadata": {},
   "source": [
    "## Function to fit an LSTM\n",
    "This function returns fitted model as well as history. History object can be used for plotting loss during training for train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc8869a-e50b-4d72-82b7-2dff4e981ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(X_train, Y_train, X_dev, Y_dev,\n",
    "               epochs, drop_rate, batch_size, decay,\n",
    "               look_back, lr, units):\n",
    "\n",
    "    num_features = X_train.shape[-1]\n",
    "    num_outputs = Y_train.shape[-1]\n",
    "    model = build_model(drop_rate,\n",
    "                        lr,\n",
    "                        units,\n",
    "                        decay,\n",
    "                        look_back,\n",
    "                        no_features=num_features,\n",
    "                        no_outputs=num_outputs)\n",
    "\n",
    "    history = model.fit(X_train,\n",
    "                        Y_train,\n",
    "                        epochs,\n",
    "                        batch_size,\n",
    "                        verbose=0,\n",
    "                        validation_data=(X_dev, Y_dev),\n",
    "                        shuffle=False)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eced9f-ff3c-485a-b47f-62cc6eac319c",
   "metadata": {},
   "source": [
    "## Train function\n",
    "Function to do training with hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf3e167-5275-4d22-86c1-8c6d35b0e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(model, x, y, scaler):\n",
    "    pred = model.predict(x)\n",
    "    dim = len(pred.shape)\n",
    "\n",
    "    if dim < 2:\n",
    "        pred = pred.reshape(-1, 1)\n",
    "\n",
    "    pred_reversed = get_inv(pred, scaler)\n",
    "    ytrue_reversed = get_inv(y, scaler)\n",
    "    ratio = hit_ratio(ytrue_reversed, pred_reversed)\n",
    "\n",
    "    return pred_reversed, ytrue_reversed, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0294f3-a8d5-4a2f-9ac3-d51fe4ec4d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, y_train, x_dev, y_dev,\n",
    "          scaler, patience=2, normalize=True):\n",
    "    \"\"\" Training function, calls train_lstm and optimizes\n",
    "    hyperparameters\"\"\"\n",
    "    count = 0\n",
    "    optimal_parameters = {}\n",
    "\n",
    "    old_hit = 0\n",
    "    accepted_hit = 0\n",
    "    accepted_mse = 0\n",
    "    old_mse = 1e6\n",
    "\n",
    "    old_mse_train = 1e6\n",
    "    old_hit_train = 0\n",
    "    accepted_hit_train = 0\n",
    "    accepted_mse_train = 0\n",
    "    \"\"\" Maybe good to save for each new better result?\"\"\"\n",
    "    while  (count < patience): # can be made mor robust\n",
    "        print('')\n",
    "        print('.' * 50)\n",
    "        print('While iteration:', count)\n",
    "        params = {'drop_rate': np.random.uniform(0.1, 0.5),\n",
    "                  'lr': 10 ** np.random.uniform(-4, -2),\n",
    "                  'units': np.random.randint(50, 200),\n",
    "                  'epochs': np.random.randint(200, 450),\n",
    "                  'decay': 1e-6,\n",
    "                  'look_back': x_train.shape[1],\n",
    "                  'batch_size': np.random.choice(\n",
    "                          np.array([2**5, 2**6, 2**7, 2**8]))}\n",
    "        print('')\n",
    "        print(params)\n",
    "        print('')\n",
    "        model, history = train_lstm(x_train, y_train, x_dev, y_dev, **params)\n",
    "        train_predict = model.predict(x_train)\n",
    "        dev_predict = model.predict(x_dev)\n",
    "\n",
    "        if np.any(np.isnan(train_predict)) or np.any(np.isinf(train_predict))\\\n",
    "        or np.any(np.isnan(y_train)) or np.any(np.isinf(y_train)):\n",
    "            print('')\n",
    "            print('check if predictions for training dataset are too big!')\n",
    "            print('are there any Nas or inf? Yes')\n",
    "            print('train_predict =', train_predict)\n",
    "            print('y_train =', y_train)\n",
    "        if np.any(np.isnan(dev_predict)) or np.any(np.isinf(dev_predict))\\\n",
    "        or np.any(np.isinf(y_dev)) or np.any(np.isinf(y_dev)):\n",
    "            print('')\n",
    "            print('check if predictions for dev dataset are too big!')\n",
    "            print('are there any Nas or inf? Yes')\n",
    "            print('dev_predict =', dev_predict)\n",
    "            print('y_dev =', y_dev)\n",
    "\n",
    "        mse_dev = mean_squared_error(y_dev, dev_predict)\n",
    "        mse_train = mean_squared_error(y_train, train_predict)\n",
    "\n",
    "        if normalize:\n",
    "            hit_train = hit_ratio(scaler.inverse_transform(y_train),\n",
    "                                  scaler.inverse_transform(train_predict),\n",
    "                                  returns=True)\n",
    "            hit_dev = hit_ratio(scaler.inverse_transform(y_dev),\n",
    "                                scaler.inverse_transform(dev_predict),\n",
    "                                returns=True)\n",
    "\n",
    "        else:\n",
    "            hit_train = hit_ratio(y_train, train_predict)\n",
    "            hit_dev = hit_ratio(y_dev, dev_predict)\n",
    "\n",
    "        # if diff betwenn train mse and dev mse\n",
    "        # less than one then no so much overfit\n",
    "        print('mse for train set:', mse_train)\n",
    "        print('mse for dev set:', mse_dev)\n",
    "        print('' )\n",
    "        print('hit-ratio train:', hit_train)\n",
    "        print('hit-ratio dev:', hit_dev)\n",
    "\n",
    "        # TODO: add sequence length as hyper parameter\n",
    "        # and then pop from optimal parameters\n",
    "        if (np.abs(mse_train - mse_dev) < .4) and (mse_train < 1. and mse_dev < 1.)\\\n",
    "        and (np.abs(hit_dev - hit_train) < 0.2):\n",
    "            old_hit = hit_dev\n",
    "            old_mse = mse_dev\n",
    "            old_hit_train = hit_train\n",
    "            old_mse_train = mse_train\n",
    "            # new code 2018-03-27\n",
    "            # added if cond and put parameter update under condition\n",
    "            if accepted_hit < old_hit and accepted_mse < old_mse:\n",
    "                \"\"\"Swap accepted and old value\"\"\"\n",
    "                accepted_hit, old_hit = old_hit, accepted_hit\n",
    "                accepted_mse, old_mse = old_mse, accepted_mse\n",
    "                accepted_mse_train, old_mse_train = old_mse_train, accepted_mse_train\n",
    "                accepted_hit_train, old_hit_train = old_hit_train, accepted_hit_train\n",
    "                print('accepted_hit:', accepted_hit)\n",
    "                optimal_parameters.update(params)\n",
    "        del model, history\n",
    "        model, history = None, None\n",
    "        if len(optimal_parameters) > 0:\n",
    "            print('')\n",
    "            print('The optimal parameters found so far', optimal_parameters)\n",
    "            print('and the results based on optimal parameters are:')\n",
    "            print('mse_train: {}, hit_train: {}, mse_dev: {}, hit_dev: {}'.format(\n",
    "                    accepted_mse_train,\n",
    "                    accepted_hit_train,\n",
    "                    accepted_mse,\n",
    "                    accepted_hit))\n",
    "        else:\n",
    "            print('')\n",
    "            print('No optimal parameters found yet')\n",
    "        del model\n",
    "        del history\n",
    "        count += 1\n",
    "        \n",
    "    print('')\n",
    "    print('The optimal parameters are:', optimal_parameters)\n",
    "    keras.backend.clear_session()\n",
    "    \"\"\" Has to retrain as the model is deleted during the\n",
    "    while loop\"\"\"\n",
    "\n",
    "    \"\"\" It can happen that the model did not find any optimal\n",
    "    parameters in that case the dict is empty. You should return\n",
    "    a message that an error happen\"\"\"\n",
    "    try:\n",
    "        assert len(optimal_parameters) > 0\n",
    "        model, history = train_lstm(x_train,\n",
    "                                    y_train,\n",
    "                                    x_dev,\n",
    "                                    y_dev,\n",
    "                                    **optimal_parameters)\n",
    "        \"\"\"\" Save model to disk in JSON form \"\"\"\n",
    "        # Serialize to JSON\n",
    "        model_json = model.to_json()\n",
    "        with open('model.json', 'w') as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights('model.h5')\n",
    "\n",
    "\n",
    "        return optimal_parameters, model, history\n",
    "    except AssertionError:\n",
    "        print('No optimal parameters were found, try again but train longer.')\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ffce6-2d3d-4556-8dc5-bd9d65118e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    count = 1\n",
    "    NORMALIZE = True\n",
    "    (x_train, y_train, x_dev, y_dev,\n",
    "     x_test, y_test, scaler, stock_names) = split_train_dev_test(cls,\n",
    "                                                                 normalize=NORMALIZE,\n",
    "                                                                 to_ret=True,\n",
    "                                                                 dev=0.9,\n",
    "                                                                 differenced=False)\n",
    "    print('')\n",
    "    print('training algorithm:')\n",
    "    print('shape train set:', x_train.shape, y_train.shape)\n",
    "    print('type of train set:', type(x_train))\n",
    "    print('shape dev set:', x_dev.shape, y_dev.shape)\n",
    "    print('shape test set:', x_test.shape, y_test.shape)\n",
    "\n",
    "    optimal_parameters, model, history = train(x_train,\n",
    "                                               y_train,\n",
    "                                               x_dev,\n",
    "                                               y_dev,\n",
    "                                               scaler,\n",
    "                                               patience=20,\n",
    "                                               normalize=NORMALIZE)\n",
    "\n",
    "    \"\"\"Both targets and predictions have to be transformed\"\"\"\n",
    "    dev_predict = model.predict(x_dev)\n",
    "    arg_pred_dev = dev_predict\n",
    "    arg_y_dev = y_dev\n",
    "\n",
    "    if NORMALIZE:\n",
    "        arg_pred_dev = scaler.inverse_transform(dev_predict)\n",
    "        arg_y_dev = scaler.inverse_transform(y_dev)\n",
    "\n",
    "    dev_predict = pd.DataFrame(arg_pred_dev, columns=stock_names)\n",
    "    y_dev = pd.DataFrame(arg_y_dev, columns=stock_names)\n",
    "\n",
    "    plot_error_curves(history)\n",
    "\n",
    "    dev_predict.plot(figsize=(10, 10))\n",
    "\n",
    "    dev_predict.iloc[:, 0:5]\n",
    "\n",
    "    y_dev.plot(figsize=(10, 10))\n",
    "\n",
    "    ax = dev_predict.iloc[:, 0:5].plot(\n",
    "            subplots=True,\n",
    "            figsize=(15, 20),\n",
    "            label='Predictions on dev-set',\n",
    "            color='DarkBlue')\n",
    "\n",
    "    y_dev.iloc[:, 0:5].plot(ax=ax,\n",
    "              subplots=True,\n",
    "               figsize=(15, 20),\n",
    "               title='Real data together with predictions',\n",
    "               label='real data',\n",
    "               color='DarkGreen')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
